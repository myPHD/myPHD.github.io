---

layout: post

title: "ISL-chap2 : Statistical Learning"

---

### 1-1. 통계학습이란?
- 용어정리
 **Input**: predictors, independent variables, features, variables
 **Output**: response, dependent variables

- 통계모델을 다음과 같이 정의하자
 
 $$$Y=f(X)+\epsilon$$$

 - $$$\epsilon$$$은 radom error term이며 평균이 0
 - 우리의 목표는 $$$f$$$를 예측하는 것

### 1-2. $$$f$$$ 함수를 예측하는 이유
#### Prediction
- 새로운 변수 $$$X$$$에 대해서 그에 따른 $$$Y$$$값 예측이 목표
- error term은 0으로 수렴하기 때문에 $$$Y$$$값을 다음과 같이 예측가능
 
 $$$\hat{Y}=\hat{f}(X)$$$
 
 - $$$\hat{f}$$$는 **black box**(미지의 것)로 여겨짐. 정확한 형태의 $$$\hat{f}$$$의형태와 관련이 없기 때문
- $$$\hat{Y}$$$의 예측정확도는 아래 두개의 항들과 관련있음
 - 오차의 제곱 평균을 다음과 같이 정의할 때
$$$E(Y-\hat{Y})^2=E(f(x)+\epsilon-\hat{f(X)})$$$
$$$=[f(X)-\hat{f}(X)]^2 + Var(\epsilon)$$$
 - 첫 항을 **reducible error**, 둘째항을 **irreducible error** 라 한다.
- 그래서 우리의 목표는 **reducible error**를 줄이는 쪽으로 진행하게 된다.

#### Inference
 - $$$Y$$$의 예측이 아닌 $$$X$$$와 $$$Y$$$의 관계를 밝혀내는 것이 목표
 - **추론(Inference)**과정에서 기초적인 4가지 질문
  1) 어떤 X가 Y와 관련이 있는가
  2) X와 Y간에 어떠한 관련이 있는가
  3) 그 관계가 선형으로 설명이 되는가? 더 복잡한가

 - 정확도 vs 편리성 의 선택에 따라서 X와 Y의 관계를 다르게 설명할 수 있다
  - 편리성을 더 따질 때는 선형모델이 더 좋을 수 있음
  - 정확도를 따질 때는 보다 복잡한 모델이 좋을 수 있음

### 1-3. $$$f$$$함수를 어떻게 예측하는가
- 예측방법은 두가지로 나눔 **parametric**과 **non-parametric**

#### parmetric Methods
- 두단계로 되어 있음
1) 함수 형태에 대한 가정하기(모델 설정하기)
2) training data를 통해서 model을 fit 또는 train하기
 - 모델을 fit 혹은 train하는 방식으로 **least square** 방식이 제일 많이 이용됨
- 단점
 - 모델이 실제 함수$$$f$$$와 안맞으면 성능이 안 좋을 수 있음
   - 이 문제를 **flexible model**을 통해서 해결하나 이를 위해서는 더 많은 **parameter**를 구하는 과정이 필요함. 그리고 **overiftting**(과적합)이 발생할수 있음

####Non-parametric Methods
- 근처에 데이터 점에 대한 정보를 이용하여 함수 $$$f$$$을 결정한다
- 모델을 설정하지 않아도 되기 때문에 **parametric method**의 단점이 해결됨
- 주변 데이터점의 몇개를 선택하느냐에 따라서 **smoothness**가 변함
- 단점
 - $$$f$$$함수를 예측하기 위해서 **parametric method**보다 훨신 더많은 양의 예측 결과가 필요함
 - 적은 개수의 주변 데이터 점을 선택하면 과적합이 발생할 수 있음=> 적절한 **smoothness**선택이 필요함

### 1-4. 예측정확도와 해석가능성 간의 관계( Prediction Accuracy vs Model Interpretability )
 - **Restrictive Models** : 모델이 restrictive 할수록 interpretable 하기 좋음, Accuracy는 낮을 수 있음
 - **flexible Models** : 모델이 flexible 할수록 interpretable 하기는 어려우나, Accuracy는 더 좋을 수 있음
  - 그러나 너무 flexible하면 overfitting(과적합)발생가능
 - Flexibility : Subset selection, Lasso < Least Squares < Generalized additive Models, Trees < SVM < Bagging, Boosting
 

### 1-5. 지도학습과 비지도학습( Supervised vs Unsupervised Learning)
- **Supervised Learning** 은 output이 있으나 **Unsupervised Learning**은 otuput이 없음
- **Unsupervised learning**은 데이터들이 서로 섞이지 않고 끼리끼리 뭉쳐져 있을 때의 분석에 적합함 (**cluster analysis**)
- **Semi-supervised learning**은 몇개의 데이터는 output이 있고 몇개의 데이터는 output이 없을 때 적합함

### 1-6. 회귀와 분류(Regression vs Classification Problems)
- 1단원에서 앞서 설명했듯이 Regression은 quantative 한 output, Classification은 qualitative한 output에서 쓰인다.
- Input이 quantitative인지 qualitative인지는 중요하지 않다(뒷 단원에서 알게 됨)
